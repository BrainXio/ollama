# This file lists environment variables for configuring the Ollama server.
# Each variable is explained in simple terms to help beginners understand what it does.
# Copy to .env, uncomment (remove the #), and set values as needed.
# Variables are ordered by how likely you are to change them, with the most common at the top.

# NAME_SUFFIX
# What it does: Adds an optional suffix to container names and Tailscale hostname.
# Example: "-small" names containers "ollama-small", "ollama-init-small", and hostname "ollama-small".
# Default: None (no suffix).
#NAME_SUFFIX=-small

# OLLAMA_BASE_MODEL
# What it does: Specifies the base model pulled by the init container.
# Example: "qwen2.5:7b-instruct-q4_K_M" for a quantized Qwen model.
# Default: smollm2
#OLLAMA_BASE_MODEL=qwen2.5:7b-instruct-q4_K_M

# OLLAMA_INIT_MODEL
# What it does: Specifies the embedding model pulled by the init container.
# Example: "nomic-embed-text" for text embedding tasks.
# Default: nomic-embed-text
#OLLAMA_INIT_MODEL=nomic-embed-text

# OLLAMA_HOST
# What it does: Sets the address and port where Ollama listens for requests.
# Example: "0.0.0.0:11434" makes Ollama accessible on all network interfaces.
# Default: 0.0.0.0:11434 (recommended for Docker).
#OLLAMA_HOST=0.0.0.0:11434

# OLLAMA_MODELS
# What it does: Specifies the folder where Ollama stores its AI models.
# Example: "/path/to/models" points to a custom folder for model storage.
# Default: ~/.ollama/models (a hidden folder in your home directory).
#OLLAMA_MODELS=/path/to/models

# OLLAMA_ORIGINS
# What it does: Controls which websites can send requests to Ollama (CORS).
# Example: "*" allows all websites; use specific URLs for security (e.g., http://example.com).
# Default: * (all origins allowed).
#OLLAMA_ORIGINS=*

# OLLAMA_DEBUG
# What it does: Turns on detailed logging to help troubleshoot issues.
# Example: "1" enables debug logs; "0" or unset keeps them off.
# Default: Disabled (0).
#OLLAMA_DEBUG=1

# OLLAMA_FLASH_ATTENTION
# What it does: Enables a feature to use less memory when processing models.
# Example: "1" turns it on; leave unset or "0" to disable.
# Default: Disabled (0).
#OLLAMA_FLASH_ATTENTION=1

# OLLAMA_GPU_OVERHEAD
# What it does: Reserves extra graphics card memory (in bytes) for Ollama’s operations.
# Example: "0" reserves no extra memory; increase if needed for stability.
# Default: 0.
#OLLAMA_GPU_OVERHEAD=0

# OLLAMA_KEEP_ALIVE
# What it does: Sets how long models stay in memory after use to speed up future requests.
# Example: "10m" keeps models loaded for 10 minutes; "-1" keeps them loaded forever.
# Default: 5m (5 minutes).
#OLLAMA_KEEP_ALIVE=5m

# OLLAMA_KV_CACHE_TYPE
# What it does: Sets the data format for memory optimization when Flash Attention is on.
# Example: "q8_0" is a specific format for efficient memory use.
# Default: None (set only if Flash Attention is enabled).
#OLLAMA_KV_CACHE_TYPE=q8_0

# OLLAMA_LOAD_TIMEOUT
# What it does: Sets the maximum time Ollama waits for a model to load.
# Example: "10m" gives 10 minutes before timing out.
# Default: 5m (5 minutes).
#OLLAMA_LOAD_TIMEOUT=10m

# OLLAMA_MAX_LOADED_MODELS
# What it does: Limits how many models can be loaded in memory at once.
# Example: "2" allows two models to be ready at the same time.
# Default: 1.
#OLLAMA_MAX_LOADED_MODELS=2

# OLLAMA_MAX_QUEUE
# What it does: Limits how many requests can wait in line when Ollama is busy.
# Example: "512" allows up to 512 requests to queue.
# Default: 512.
#OLLAMA_MAX_QUEUE=512

# OLLAMA_NOHISTORY
# What it does: Disables saving command history in Ollama’s interactive mode.
# Example: "1" turns off history; unset or "0" keeps it on.
# Default: Disabled (history is saved).
#OLLAMA_NOHISTORY=1

# OLLAMA_NOPRUNE
# What it does: Stops Ollama from deleting unused model data on startup.
# Example: "1" keeps all data; unset or "0" allows cleanup.
# Default: Disabled (pruning enabled).
#OLLAMA_NOPRUNE=1

# OLLAMA_NUM_PARALLEL
# What it does: Sets how many requests per model can run at the same time.
# Example: "4" allows 4 simultaneous requests per model.
# Default: 1.
#OLLAMA_NUM_PARALLEL=4

# HTTPS_PROXY
# What it does: Specifies a proxy server for downloading models over HTTPS.
# Example: "https://proxy.example.com:8080" routes model downloads through a proxy.
# Default: None (direct connection).
#HTTPS_PROXY=https://proxy.example.com:8080